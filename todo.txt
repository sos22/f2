filesystem = f, compute = c, testing = t, storage = s, infra = X

c2) Extend compute slave interface with a way of turning jobs into
    running tasks.  Tasks will hang around in memory indefinitely
    after they terminate.  Tasks will be identified by the job which
    created them i.e. no way of distinguishing two instances of job X
    (might add something for debugging, but semantically they're
    indistinguishable)
c3) CLI interface for creating hello-world tasks directly against a
    compute slave.  The message will include the peername which holds
    the task.
c4) CLI interface for getting the results of a task.  That'll just be
    a state enumeration, plus a success/failure flag (at this stage).
c5) CLI interface for deleting old results from the slave's task
    table.
f4) Coordinator starts tracking compute slaves.  This should be pretty
    trivial; just adjust beacon client config.
f5) Coordinator takes an inventory of compute slaves as they turn up.
    It uses that to build a running job name -> compute slave mapping.
    This'll be a separate structure to the object name -> storage
    slave mapping.  For a start, it should be a single map, because we
    shouldn't have multiple slaves building the same thing at once.
c6) Compute slaves expose a poll interface for getting notified when
    tasks change state.  This could be an event queue or it could be a
    single posted RPC; not sure yet.  I like the symmetry of using
    queues for both kinds of slave, but I'm not sure the complexity is
    needed here, because the number of running tasks on a compute
    slave should be far smaller than the number of objects on a
    storage slave.
c7) CLI for compute slave task poll interface.
f6) Coordinator polls compute slaves to detect when tasks complete.
    At a minimum it needs to update its task table.  If the task
    failed softly and we still need the results then the job goes back
    in the pending queue.  If we no longer need the results we just
    discard it.  If it failed hard or succeeded then we don't need to
    do anything (all of the interesting bits are keyed off of the
    output objects, not the task results).  If it just disappeared we
    need to do some scavenging and zap partial results on storage
    slaves.  Also need to do that if partial objects turn up on
    storage slaves unexpectedly.  Otherwise treat disappearance as a
    soft failure.
f7) Coordinator keeps track of how busy each compute slave is.
    Probably a simple count of running tasks for now.
f8) Coordinator maintains an index of incomplete jobs showing how long
    each has been waiting.  This'll be purely in memory and will be
    maintained in response to storage slaves notifying us about things
    arriving.  Initial version will be very simple; might even just
    walk the entire filesystem table on every query.
f9) Coordinator notices when compute slaves go idle and sends them the
    oldest incomplete job.  That's not quite in the model (which
    should be demand-driven), but it's still pretty reasonable, and
    it's easier to implement than the real thing.  Might want to add a
    bit of debounce, so we don't schedule things eagerly unless
    compute slaves have been idle for a while, but that can wait.  The
    coordinator interface should update the running job table eagerly,
    before it returns, rather than relying on notifications from the
    compute slaves.  Still need them for failure tolerance, it's just
    that not using them makes the non-failure behaviour a bit clearer.
c8) Add immediate inputs to the job specification and the compute
    slave.  Extend hello-world to use them.  This should be pretty
    trivial.
c9) Indirect (i.e. stored in objects) outputs for tasks.  Whoever
    starts the task must already have created an appropriate object on
    the storage slave to hold the incomplete job.  The peername of
    that storage slave will be included in the start-task message.
    Failure to contact the storage slave will fail the task.  Populate
    and finish messages will be sent directly from the compute slave.
    The task state won't change to finished until the storage slaves
    have ACKed all of the stream-complete messages.
c10) Support for indirect inputs to jobs.  Inputs can now be specified
    by object name, rather than being inlined into the job spec.  The
    message to the compute slave includes the storage peername which
    currently holds the input; if that's wrong, the task fails.  If
    the input object's not complete by the time the task starts then
    the task fails.  First version will pass the storage peername to
    the task, rather than fetching from the storage.  This is just the
    job spec bits and the slave -> peername lookup.
f10) Dependency tracking for tasks.  When the coordinator discovers a
    new runnable job it checks what objects it depends on and won't
    add it to the queue until all of the objects are available.  When
    an object becomes available the coordinator checks whether that
    makes any more jobs runnable.  The object completion handling will
    be driven off of the notifications from storage slaves, not off of
    task complete notifications from compute slaves.  Dependencies
    will be added when the coordinator discovers incomplete jobs,
    whether that's an async watch or an explicit introduction.
c11) Proper (i.e. object content) indirect inputs to jobs.  Compute
    slave does appropriate RPCs to the nominated storage slave before
    starting the task to grab a copy of the entire object (initially;
    we'll do streaming later).
f11) Demand-driven scheduling.  Coordinator tracks how many jobs are
    blocked on each job and tries to schedule the most useful jobs
    first.  This only affects the bit where we decide what to run
    next; no suspend or abort operations yet.
s3) Storage slave support for delegation.  In addition to the empty,
    partial and finished states streams will now have a delegated
    state in which they point at an output stream from another job.
c12) Compute slave support for delegation.  Rather than doing the
    delegation directly, the slave will have to tell the master to do
    it.  Putting it in the task result structure seems like the
    obvious answer, at least in the first instance.  The task will
    include a complete job spec for the target job, not just its
    handle, and a stream name.  At some point we'll need an event
    queue so that you can delegate before the job finishes, but
    that can wait.
f12) Coordinator support for delegation.  When a task completes with a
    delegation we go and create an appropriate incomplete job on some
    storage slave or other, add the job to the coordinator incomplete
    tasks table, update the original job's result record on the
    original storage slave, add the new job to the coordinator
    incomplete job table, and then bump any jobs which were waiting on
    the old job over so that they're waiting on the new one.  The
    compute slave can't create the new job itself because it doesn't
    know where to put it (and it can't ask the master because of the
    deadlock avoidance rules), which means that it can't update the
    old task's storage slave itself.  This looks like it's going to be
    a fairly complex operation.


Deadlock avoidance: filesystem can call into compute and storage,
compute can call into storage, storage can't call anything.  Calls the
other way around have to be posted.

Also: poll-type calls need to have some way of bypassing admission
control.  e.g. if the compute slave is waiting for a thousand tasks to
complete it should be able to created a thousand posted calls against
the relevant storage slave.

-- Compute slave failure recovery.
-- Storage slave failure recovery.
-- Master failure recovery.
-- Crapton of testing.
-- Some kind of benchmarking.
-- Start publicising it a little bit.

-- The domain logger thing.
