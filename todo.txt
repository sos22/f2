filesystem = f, compute = c, testing = t, storage = s, infra = X

-- Something analogous to the filesystem, but tracking the contents of
   compute agents rather than storage ones.  Call it the monitor until
   I think of a better name.  This'll be a component of the
   coordinator daemon.

-- Compute agent needs to be able to run non-trivial jobs, even if
   it's just invoking them in isolation.

-- Extend job type with a way of specifying output streams.

-- Compute agents contact the filesystem to find out where those
   output streams live.  This'll be pretty trivial at the moment,
   since they all live in the same place, but it might be handy later.

-- Compute agent exposes something to the tasks which allows them to
   actually populate the output streams.

-- Have the filesystem expose an event queue indicating when streams
   disappear or become available.

f4) Coordinator starts tracking compute slaves.  This should be pretty
    trivial; just adjust beacon client config.
f5) Coordinator takes an inventory of compute slaves as they turn up.
    It uses that to build a running job name -> compute slave mapping.
    This'll be a separate structure to the object name -> storage
    slave mapping.  For a start, it should be a single map, because we
    shouldn't have multiple slaves building the same thing at once.
f6) Coordinator polls compute slaves to detect when tasks complete.
    At a minimum it needs to update its task table.  If the task
    failed softly and we still need the results then the job goes back
    in the pending queue.  If we no longer need the results we just
    discard it.  If it failed hard or succeeded then we don't need to
    do anything (all of the interesting bits are keyed off of the
    output objects, not the task results).  If it just disappeared we
    need to do some scavenging and zap partial results on storage
    slaves.  Also need to do that if partial objects turn up on
    storage slaves unexpectedly.  Otherwise treat disappearance as a
    soft failure.
f7) Coordinator keeps track of how busy each compute slave is.
    Probably a simple count of running tasks for now.
f8) Coordinator maintains an index of incomplete jobs showing how long
    each has been waiting.  This'll be purely in memory and will be
    maintained in response to storage slaves notifying us about things
    arriving.  Initial version will be very simple; might even just
    walk the entire filesystem table on every query.
f9) Coordinator notices when compute slaves go idle and sends them the
    oldest incomplete job.  That's not quite in the model (which
    should be demand-driven), but it's still pretty reasonable, and
    it's easier to implement than the real thing.  Might want to add a
    bit of debounce, so we don't schedule things eagerly unless
    compute slaves have been idle for a while, but that can wait.  The
    coordinator interface should update the running job table eagerly,
    before it returns, rather than relying on notifications from the
    compute slaves.  Still need them for failure tolerance, it's just
    that not using them makes the non-failure behaviour a bit clearer.
c8) Add immediate inputs to the job specification and the compute
    slave.  Extend hello-world to use them.  This should be pretty
    trivial.
c9) Indirect (i.e. stored in objects) outputs for tasks.  Whoever
    starts the task must already have created an appropriate object on
    the storage slave to hold the incomplete job.  The peername of
    that storage slave will be included in the start-task message.
    Failure to contact the storage slave will fail the task.  Populate
    and finish messages will be sent directly from the compute slave.
    The task state won't change to finished until the storage slaves
    have ACKed all of the stream-complete messages.
c10) Support for indirect inputs to jobs.  Inputs can now be specified
    by object name, rather than being inlined into the job spec.  The
    message to the compute slave includes the storage peername which
    currently holds the input; if that's wrong, the task fails.  If
    the input object's not complete by the time the task starts then
    the task fails.  First version will pass the storage peername to
    the task, rather than fetching from the storage.  This is just the
    job spec bits and the slave -> peername lookup.
f10) Dependency tracking for tasks.  When the coordinator discovers a
    new runnable job it checks what objects it depends on and won't
    add it to the queue until all of the objects are available.  When
    an object becomes available the coordinator checks whether that
    makes any more jobs runnable.  The object completion handling will
    be driven off of the notifications from storage slaves, not off of
    task complete notifications from compute slaves.  Dependencies
    will be added when the coordinator discovers incomplete jobs,
    whether that's an async watch or an explicit introduction.
c11) Proper (i.e. object content) indirect inputs to jobs.  Compute
    slave does appropriate RPCs to the nominated storage slave before
    starting the task to grab a copy of the entire object (initially;
    we'll do streaming later).
f11) Demand-driven scheduling.  Coordinator tracks how many jobs are
    blocked on each job and tries to schedule the most useful jobs
    first.  This only affects the bit where we decide what to run
    next; no suspend or abort operations yet.
s3) Storage slave support for delegation.  In addition to the empty,
    partial and finished states streams will now have a delegated
    state in which they point at an output stream from another job.
c12) Compute slave support for delegation.  Rather than doing the
    delegation directly, the slave will have to tell the master to do
    it.  Putting it in the task result structure seems like the
    obvious answer, at least in the first instance.  The task will
    include a complete job spec for the target job, not just its
    handle, and a stream name.  At some point we'll need an event
    queue so that you can delegate before the job finishes, but
    that can wait.
f12) Coordinator support for delegation.  When a task completes with a
    delegation we go and create an appropriate incomplete job on some
    storage slave or other, add the job to the coordinator incomplete
    tasks table, update the original job's result record on the
    original storage slave, add the new job to the coordinator
    incomplete job table, and then bump any jobs which were waiting on
    the old job over so that they're waiting on the new one.  The
    compute slave can't create the new job itself because it doesn't
    know where to put it (and it can't ask the master because of the
    deadlock avoidance rules), which means that it can't update the
    old task's storage slave itself.  This looks like it's going to be
    a fairly complex operation.


Deadlock avoidance: filesystem can call into compute and storage,
compute can call into storage, storage can't call anything.  Calls the
other way around have to be posted.

-- Master failure recovery.
-- Crapton of testing.
-- Some kind of benchmarking.
-- Start publicising it a little bit.

-- The domain logger thing.
