/* Client half of the event queue system.  The main guarantee an event
 * queue gives to its caller is that events are never dropped
 * silently: if the client connects at time A, the server generates an
 * event at time B and the client checks for events at time C, then
 * provided B is after A and C is sufficiently after B, C will either
 * receive the event or will receive an error indication.  The exact
 * meaning of ``sufficiently after'' is left unspecificied, beyond
 * specifying that the delay always be finite.  Note that the queue
 * can occasionally generate spurious errors, even when no events have
 * been dropped and there have been no ``actual'' errors, although we
 * obviously try to keep spurious errors sufficently rare not to
 * render the queue completely useless. */
#ifndef EQCLIENT_H__
#define EQCLIENT_H__

#include "buffer.H"
#include "clientio.H"
#include "connpool.H"
#include "eq.H"
#include "maybe.H"
#include "pair.H"
#include "pubsub.H"
#include "serialise.H"
#include "timedelta.H"
#include "timestamp.H"

#include "connpool.tmpl"

template <typename> class nnp;
template <typename> class orerror;
class agentname;
namespace tests { template <typename, typename...> class hookpoint; }

class eqclientconfig {
private: eqclientconfig();
    /* How long to wait for the server to respond to unsubscribe
     * messages before we just give up and shut down anyway.  It
     * should usually be short; seconds at most. */
public:  timedelta unsubscribe;
    /* Timeout on get requests before we conclude that the queue has
     * died and indicate an error to the higher levels.  This is the
     * minimum time the server must be uncontactable for before we
     * give up; the maximum is this plus the wait time.  This should
     * usually be on the order of a few seconds. */
public:  timedelta get;
    /* Timeout on wait requests before we give up and switch to
     * running get again.  Setting a short timeout here will allow us
     * to detect dead servers more quickly but will increase the
     * number of messages we need to send.  This should usually be on
     * the order of a minute or so. */
public:  timedelta wait;
    /* Maximum number of events to queue up on the client side (in
     * addition to the queue maintained on the server side).  This is
     * there to cover the gap between the queue thread receiving an
     * event and something higher up picking it up.  Making it too
     * small risks unnecessarily dropping events; making it too big
     * wastes memory and risks buffer bloat-like problems by making it
     * harder to detect when the client is overloaded.  Something on
     * the order of a few hundred is usually reasonable.  Less than
     * two will cause unpredictable behaviour. */
public:  unsigned maxqueue;
    /* Generate a reasonable default configuration. */
public:  static eqclientconfig dflt(); };

/* Implementation detail of eqclient. */
class geneqclient {
private: class impl;
    friend class impl;
public:  impl &implementation();
private: const impl &implementation() const;
public:  void start(clientio);
public:  static orerror<nnp<geneqclient> > connect(
    clientio io,
    connpool &pool,
    const agentname &sn,
    const proto::eq::genname &,
    timestamp deadline,
    const eqclientconfig &config);
public:  class asyncconnect {
    public:  class impl;
        friend class impl;
    public:  class token {
            friend class geneqclient::asyncconnect;
        private: typedef connpool::asynccallT<pair<proto::eq::subscriptionid,
                                                   proto::eq::eventid> >::token
             innerT;
        private: innerT inner;
        private: explicit token(const innerT &_inner)
            : inner(_inner) {}; };
    public:  impl &implementation();
    public:  const impl &implementation() const;
    public:  const publisher &pub() const;
    public:  maybe<token> finished() const;
    public:  orerror<nnp<geneqclient> > pop(token);
    public:  void abort(); };
public:  static nnp<asyncconnect> connect(
    connpool &pool,
    const agentname &sn,
    const proto::eq::genname &,
    const eqclientconfig &);
public:  const publisher &pub() const;
public:  maybe<orerror<pair<proto::eq::eventid, buffer> > > pop();
public:  void destroy();
private: ~geneqclient();
public:  static tests::hookpoint<void> startingwaiter; };

template <typename t> class eqclient final : private geneqclient {
private: eqclient();

    /* Note that this cannot have any members, because its destructor
     * is never invoked. */

    /* ``Connect'' to a remote event queue.  The semi-reliability
     * guarantee only applies to events generated by the server after
     * this returns.  This always succeeds; errors are reported at the
     * first call to pop(). */
public:  static orerror<nnp<eqclient> > connect(
    clientio io,
    connpool &pool,
    const agentname &sn,
    const proto::eq::name<t> &queue,
    timestamp d,
    const eqclientconfig &config = eqclientconfig::dflt()) {
        auto r(geneqclient::connect(io, pool, sn, queue, d, config));
        if (r.isfailure()) return r.failure();
        else return _nnp(*static_cast<eqclient *>(&*r.success())); }

    /* Async connect interface.  Start connecting to a remote queue,
     * but rather than waiting return an asyncconnect structure which
     * can be used to collect the results later. */
public:  class asyncconnect : private geneqclient::asyncconnect {
    private: asyncconnect(const asyncconnect &) = delete;
    private: void operator=(const asyncconnect &) = delete;
    public:  class token {
            friend class eqclient<t>::asyncconnect;
        private: geneqclient::asyncconnect::token inner;
        private: explicit token(geneqclient::asyncconnect::token &_inner)
            : inner(_inner) {} };
        /* Use connect() */
    private: asyncconnect();
        /* Use pop() or abort() */
    private: ~asyncconnect();
        /* Notified when finished() becomes non-Nothing. */
    public:  const publisher &pub() const {
        return geneqclient::asyncconnect::pub(); }
        /* Returns Nothing if the connect is still outstanding or
         * a token if it's completed. */
    public:  maybe<token> finished() const {
        auto r(geneqclient::asyncconnect::finished());
        if (r == Nothing) return Nothing;
        else return token(r.just()); }
        /* Get the result of the connect (either an eqclient or an
         * error), once finished() returns non-Nothing.  Destroys the
         * asyncconnect structure in the process (so you must be
         * unsubscribed from the publisher before calling this).*/
    public:  orerror<nnp<eqclient> > pop(token tok) {
        auto r(geneqclient::asyncconnect::pop(tok.inner));
        if (r.isfailure()) return r.failure();
        else return _nnp(*static_cast<eqclient *>(&*r.success())); }
        /* Give up trying to connect to the queue and destroy the
         * asyncconnect. */
    public:  void abort() { geneqclient::asyncconnect::abort(); } };
public:  static nnp<asyncconnect> connect(
    connpool &pool,
    const agentname &sn,
    const proto::eq::name<t> &queue,
    const eqclientconfig &config = eqclientconfig::dflt()) {
        return _nnp(*reinterpret_cast<asyncconnect *>(
                        &*geneqclient::connect(pool, sn, queue, config))); }

    /* Notified whenever pop() becomes non-Nothing. */
public:  const publisher &pub() const { return geneqclient::pub(); }

    /* Extract the next thing from the queue, without blocking, and
     * return Nothing if nothing is currently available.  Can also
     * return an error if the queue has failed.  Errors are sticky:
     * the only way to clear them is to tear the queue down and start
     * again. */
public:  maybe<orerror<pair<proto::eq::eventid, t> > > popid() {
        auto r(geneqclient::pop());
        if (r == Nothing) return Nothing;
        else if (r.just().isfailure()) {
            return maybe<orerror<pair<proto::eq::eventid, t> > >(
                r.just().failure()); }
        else {
            deserialise1 ds(r.just().success().second());
            t res(ds);
            if (ds.isfailure()) {
                return orerror<pair<proto::eq::eventid, t> >(ds.failure()); }
            else return success(mkpair(r.just().success().first(), res)); } }

    /* Blocking variant of popid().  Equivalent to calling popid()
     * repeatedly until it returns non-Nothing. */
public:  orerror< pair<proto::eq::eventid, t> > popid(clientio io) {
        auto res(popid());
        if (res == Nothing) {
            subscriber sub;
            subscription ss(sub, pub());
            res = popid();
            while (res == Nothing) {
                sub.wait(io);
                res = popid(); } }
        return res.just(); }

    /* Variants of popid() when you don't care about the event id. */
public:  maybe<orerror<t> > pop() {
        auto r(popid());
        if (r == Nothing) return Nothing;
        else if (r.just().isfailure()) return orerror<t>(r.just().failure());
        else return success(r.just().success().second()); }
public:  orerror<t> pop(clientio io) {
        auto r(popid(io));
        if (r.isfailure()) return r.failure();
        else return r.success().second(); }

    /* Tear down a queue client, discarding any pending events not yet
     * returned by pop(). */
public:  void destroy() { geneqclient::destroy(); }

    /* Never actually called: base class destructor is non-virtual. */
private: ~eqclient() = delete; };

#endif /* !EQCLIENT_H__ */
